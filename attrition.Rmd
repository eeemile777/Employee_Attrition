---
title: "Attrition"
output: html_document
date: "2023-08-17"
---



```{r}
# Clear all variables in the workspace
rm(list = ls())
```



# Libraries, Data Loading and Merging: 
```{r}
library(factoextra)
library(reshape2)
library(randomForest)
library(gridExtra)
library(readr)
library(dplyr)
library(purrr)
library(ggplot2)
library(car)
library(corrplot)
library(e1071)
library(MASS)
library(pls)
library(splines)
library(caret)
library(leaps)
library(gam)
library(MLmetrics)
library(cluster)
library(rpart)
library(rpart.plot)
library(factoextra)
library(plotly)
library("igraph")
library("ggraph")
library(fastDummies)
library(pROC)
library(partykit)
library(class)
library(tidyr)
library(ggfortify)
```

the dataset was already splitted for training and test sets
i chose to merge them to have more datapoints

```{r}
test <- read_csv("/Users/milo/Desktop/datasets/test.csv")
train <- read_csv("/Users/milo/Desktop/datasets/train.csv")

# Remove the first column (ID column) from the test dataset
test <- test[, -1]


# Combine the training and test datasets into one
employee <- rbind(train, test)

```
```{r}
str(employee)
```


#Data Cleaning and Preprocessing

- NA
```{r}
# Check for missing values in the employee dataset 
missing_values <- colSums(is.na(employee))

# Display the number of missing values in each column
print("Missing Values in Employee Dataset:")
print(missing_values)

```

- Target Variable
```{r}
ggplot(employee, aes(x = factor(Attrition))) +
  geom_bar() +
  labs(title = "Distribution of Attrition",
       x = "Attrition",
       y = "Count")

```

The class distribution illustrates a significant imbalance, with a notably larger number of employees in the "No Attrition" class (Class 0) compared to the "Attrition" class (Class 1).

Downsampling involves reducing the size of the majority class (Class 0) to match the size of the minority class (Class 1). 

```{r}
minority_class <- 1  #class 1 is the minority class

# Randomly select a subset of majority class samples to match the size of the minority class
set.seed(42)  # Set a seed for reproducibility
majority_class_indices <- which(employee$Attrition == 0)
sampled_majority_indices <- sample(majority_class_indices, sum(employee$Attrition == minority_class))

# Create a downsampled dataset
employee <- employee[c(sampled_majority_indices, which(employee$Attrition == minority_class)), ]

# Check the class distribution in the downsampled dataset
ggplot(employee, aes(x = factor(Attrition))) +
  geom_bar() +
  labs(title = "Distribution of Attrition",
       x = "Attrition",
       y = "Count")

```
This balanced class distribution will be beneficial for building machine learning models as it can help prevent model bias and lead to more accurate predictions for both classes.


I hate uppercases
```{r}
# Convert column names to lowercase
colnames(employee) <- tolower(colnames(employee))
# Check the updated column names
colnames(employee)
```

# Explanatory Data Analysis

## Numerical variables
```{r}
str(employee)
```

```{r}
# Create a list of numerical variable names
numerical_vars <- c(
  "satisfaction_level",
  "last_evaluation_rating",
  "projects_worked_on",
  "average_montly_hours",
  "time_spend_company"
)

# Loop through the numerical variables and create histograms
histograms <- lapply(numerical_vars, function(var) {
  ggplot(employee, aes(x = get(var))) +
    geom_histogram(binwidth = 1, fill = "blue", color = "black") +
    labs(title = paste("Distribution of", var),
         x = var,
         y = "Count")
})

# Arrange the histograms in a grid
grid.arrange(grobs = histograms, ncol = 2)  

```


```{r}
# Satisfaction Level by Attrition
ggplot(employee, aes(x = factor(attrition), y = satisfaction_level, fill = factor(attrition))) +
  geom_boxplot() +
  labs(title = "Satisfaction Level by Attrition",
       x = "Attrition",
       y = "Satisfaction Level") +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("No", "Yes"))

# Last Evaluation Rating by Attrition
ggplot(employee, aes(x = factor(attrition), y = last_evaluation_rating, fill = factor(attrition))) +
  geom_boxplot() +
  labs(title = "Last Evaluation Rating by Attrition",
       x = "Attrition",
       y = "Last Evaluation Rating") +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("No", "Yes"))

# Projects Worked On by Attrition
ggplot(employee, aes(x = factor(attrition), y = projects_worked_on, fill = factor(attrition))) +
  geom_boxplot() +
  labs(title = "Projects Worked On by Attrition",
       x = "Attrition",
       y = "Projects Worked On") +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("No", "Yes"))

# Average Monthly Hours by Attrition
ggplot(employee, aes(x = factor(attrition), y = average_montly_hours, fill = factor(attrition))) +
  geom_boxplot() +
  labs(title = "Average Monthly Hours by Attrition",
       x = "Attrition",
       y = "Average Monthly Hours") +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("No", "Yes"))

# Time Spend Company by Attrition
ggplot(employee, aes(x = factor(attrition), y = time_spend_company, fill = factor(attrition))) +
  geom_boxplot() +
  labs(title = "Time Spent at Company by Attrition",
       x = "Attrition",
       y = "Time Spent at Company (Years)") +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("No", "Yes"))


```


## Categorical Variables
```{r}
# Department (Categorical) 
ggplot(employee, aes(x = department, fill = factor(attrition))) +
  geom_bar(position = "dodge") + 
  labs(title = "Distribution of Employees by Department by Attrition",
       x = "Department",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(legend.position = "right")

# Salary (Categorical) 
ggplot(employee, aes(x = salary, fill = factor(attrition))) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Salary Levels by Attrition",
       x = "Salary Level",
       y = "Count") +
  theme(legend.position = "right")

```

```{r}
unique(employee$department)
```

```{r}
# Create a function to map departments to broader categories
map_department <- function(department) {
  if (department %in% c("product_mng", "sales", "marketing")) {
    return("sales_marketing")
  } else if (department %in% c("IT", "technical", "RandD","support")) {
    return("tech")
  } else if (department %in% c("hr", "management", "accounting")) {
    return("admin_and_managment")
  } else {
    return("other")
  }
}


# Apply the mapping function to update the column and remove names
employee$department <- unname(sapply(employee$department, map_department))

# Check the unique values after
unique_departments <- unique(employee$department)
print(unique_departments)

```
The reason for doing this could be to simplify the analysis or visualization by reducing the number of distinct department categories.


```{r}
ggplot(employee, aes(x = department, fill = factor(attrition))) +
  geom_bar(position = "dodge") + 
  labs(title = "Distribution of Employees by Department by Attrition",
       x = "Department",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(legend.position = "right")
```

 

```{r}
# Work Accident by Attrition 
ggplot(employee, aes(x = factor(work_accident), fill = factor(attrition))) +
  geom_bar(position = "dodge") +  
  labs(title = "Work Accident Frequency by Attrition",
       x = "Work Accident",
       y = "Frequency") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  theme(legend.position = "right")

# Promotion in the Last 5 Years by Attrition 
ggplot(employee, aes(x = factor(promotion_last_5years), fill = factor(attrition))) +
  geom_bar(position = "dodge") +  
  labs(title = "Promotion in the Last 5 Years by Attrition",
       x = "Promotion in the Last 5 Years",
       y = "Frequency") +
  scale_x_discrete(labels = c("No", "Yes")) + 
  theme(legend.position = "right")


```



## Correlation analysis

```{r}
numerical_columns <- employee[, sapply(employee, is.numeric)]

correlation_matrix <- cor(numerical_columns)

correlation_matrix
```
```{r}
library(ggcorrplot)
ggcorrplot(correlation_matrix,
           hc.order = TRUE,
           type = "lower",
           lab = TRUE)
```
Satisfaction Level (-0.42): This is the most significant correlation. Lower job satisfaction is strongly associated with higher attrition rates. Improving employee satisfaction may help reduce attrition.

Time Spend at the Company (0.17): There is a modest positive correlation. Longer-tenured employees are somewhat more likely to leave, but the relationship is not very strong.

```{r}
model<-glm(attrition ~. ,data=employee)

vif<- vif(model)
vif


```
No multicollinearity





# Encoding and Scaling

## Categorical Encoding

Salary: Ordinal Encoding ---- low < medium < high
Department: One-Hot Encoding

```{r}
# Define the order for ordinal encoding of 'salary'
salary_order <- c("low", "medium", "high")

# Ordinal encode the 'salary' column and One-hot encode "department
employee_encoded <- employee %>%
  mutate(salary = factor(salary, levels = salary_order, ordered = TRUE)) %>%
  dummy_cols(select_columns = "department")

head(employee_encoded)
```

## Numerical Scaling



```{r}
# Remove the "department" column
employee_standardized <- employee_encoded %>%
  select(-department)

# Select the numerical columns you want to standardize
numerical_cols <- c(
  "satisfaction_level",
  "last_evaluation_rating",
  "projects_worked_on",
  "average_montly_hours",
  "time_spend_company"
)

# Loop through each numerical column and standardize it
for (col in numerical_cols) {
  employee_standardized[[col]] <- (employee_standardized[[col]] - mean(employee_standardized[[col]])) / sd(employee_standardized[[col]])
}


```


-Removing one of the dummies

```{r}
# Remove one of the dummy columns (e.g., department_admin_and_managment)
employee_standardized <- employee_standardized %>%
  select(-department_admin_and_managment)
head(employee_standardized)
```

convert binary to factor:

```{r}
employee_supervised <- employee_standardized
employee_supervised$attrition <- factor(employee_encoded$attrition, levels = c(0, 1))
employee_supervised$work_accident <- factor(employee_encoded$work_accident, levels = c(0, 1))
employee_supervised$promotion_last_5years <- factor(employee_encoded$promotion_last_5years, levels = c(0, 1))
employee_supervised$department_sales_marketing <- factor(employee_encoded$department_sales_marketing, levels = c(0, 1))
employee_supervised$department_tech <- factor(employee_encoded$department_tech, levels = c(0, 1))
```







#Splitting

```{r}
# Split the data into training (80%) and testing (20%) sets
set.seed(123) # for reproducibility
split_index <- createDataPartition(employee_supervised$attrition, p = 0.8, list = FALSE)
train_data <- employee_supervised[split_index, ]
test_data <- employee_supervised[-split_index, ]
```


```{r}
dim(train_data)
dim(test_data)
```


# Supervised Learning

##Logistic Regression

```{r}
# Fit a Logistic Regression model
model_logistic <- glm(attrition ~ . , data = train_data, family = binomial)

# Summary of the logistic regression model
summary(model_logistic)

```


### Metrics

```{r}
# Make predictions on the training data
train_predictions_log <- predict(model_logistic, newdata = train_data, type = "response")


# Create a confusion matrix for the training set
train_conf_matrix_log <- confusionMatrix(data = as.factor(round(train_predictions_log)), 
                                     reference = as.factor(train_data$attrition))


train_precision_log <- train_conf_matrix_log$byClass["Pos Pred Value"]
train_recall_log <- train_conf_matrix_log$byClass["Sensitivity"]
train_f1_score_log <- (2 * train_precision_log * train_recall_log) / (train_precision_log + train_recall_log)



print("Train:")
train_conf_matrix_log$overall["Accuracy"]
cat("F1-score:", train_f1_score_log, "\n")


# Make predictions on the test data
test_predictions_log <- predict(model_logistic, newdata = test_data, type = "response")

# Create a confusion matrix
test_conf_matrix_log <- confusionMatrix(data = as.factor(round(test_predictions_log)), 
                                reference = as.factor(test_data$attrition))

precision_log <- test_conf_matrix_log$byClass["Pos Pred Value"]
recall_log <- test_conf_matrix_log$byClass["Sensitivity"]
f1_score_log <- (2 * precision_log * recall_log) / (precision_log + recall_log)



print("Test:")
test_conf_matrix_log$overall["Accuracy"]
cat("F1-score:", f1_score_log, "\n")
```


### Feature Importance

```{r}
coefficients_log <- coef(model_logistic)

# Sort the coefficients by absolute value in descending order
sorted_coefficients_log <- coefficients_log[order(abs(coefficients_log), decreasing = TRUE)]

# Keep the top 6 coefficients
top_coefficients_log <- head(sorted_coefficients_log, 6)

# Create a data frame for plotting
plot_data_log <- data.frame(Feature = names(top_coefficients_log), Importance = abs(top_coefficients_log))
# Find the row where Feature is "salary.L" and replace it with "salary"
plot_data_log$Feature[plot_data_log$Feature == "salary.L"] <- "salary"


# Scale the importance values
plot_data_log$Importance <- plot_data_log$Importance / max(plot_data_log$Importance)

# Create a bar plot
ggplot(plot_data_log, aes(x = Feature, y = Importance, fill = Feature)) +
  geom_bar(stat = "identity") +
  labs(x = "Feature", y = "Scaled Importance", title = "Top 6 Scaled Variable Importances (Logistic Regression)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```


### Subset
```{r}
# Load the necessary library for cross-validation
library(caret)

# Define the training control with 10-fold cross-validation
train_control_subset <- trainControl(method = "cv", number = 10)

# Train the logistic regression model with cross-validation
model_logistic_cv_subset <- train(
  attrition ~ satisfaction_level + time_spend_company + promotion_last_5years + work_accident + salary ,
  data = train_data,
  method = "glm",
  family = binomial,
  trControl = train_control_subset
)

# View the cross-validation results
summary(model_logistic_cv_subset)
```

```{r}
# Make predictions on the training data
train_predictions_log_subset <- predict(model_logistic_cv_subset, newdata = train_data, type = "raw")


train_predictions_log_subset <- factor(train_predictions_log_subset,  
  levels = levels(train_data$attrition)
)

# Create the confusion matrix
train_conf_matrix_log_subset <- confusionMatrix(data = train_predictions_log_subset, 
                                         reference = train_data$attrition)

# Calculate performance metrics (precision, recall, F1-score, etc.) as needed


# Make predictions on the test data
test_predictions_log_subset <- predict(model_logistic_cv_subset, newdata = test_data, type = "raw")
test_predictions_log_subset <- factor(test_predictions_log_subset, levels = levels(test_data$attrition))


# Create a confusion matrix
test_conf_matrix_log_subset <- confusionMatrix(data = test_predictions_log_subset, reference = test_data$attrition)





train_precision_log_subset <- train_conf_matrix_log_subset$byClass["Pos Pred Value"]
train_recall_log_subset <- train_conf_matrix_log_subset$byClass["Sensitivity"]
train_f1_score_log_subset <- (2 * train_precision_log_subset * train_recall_log_subset) / (train_precision_log_subset + train_recall_log_subset)

print("Train:")
train_conf_matrix_log_subset$overall["Accuracy"]
cat("F1-score:", train_f1_score_log_subset, "\n")





precision_log_subset <- test_conf_matrix_log_subset$byClass["Pos Pred Value"]
recall_log_subset <- test_conf_matrix_log_subset$byClass["Sensitivity"]
f1_score_log_subset <- (2 * precision_log_subset * recall_log_subset) / (precision_log_subset + recall_log_subset)

print("Test:")
test_conf_matrix_log_subset$overall["Accuracy"]
cat("F1-score:", f1_score_log_subset, "\n")
```



##Random Forest

```{r}
# Train the Random Forest model using all columns
set.seed(123)  # for reproducibility
rf_model <- randomForest(attrition ~ ., data = train_data, importance=TRUE, ntree = 100)

# Print the model summary
print(rf_model)
```

### Metrics
```{r}
# Make predictions on the training data
train_predictions_rf <- predict(rf_model, newdata = train_data, type = "class")

# Create a confusion matrix for the training set
train_conf_matrix_rf <- confusionMatrix(data = train_predictions_rf, 
                                     reference = train_data$attrition)

train_precision_rf <- train_conf_matrix_rf$byClass["Pos Pred Value"]
train_recall_rf <- train_conf_matrix_rf$byClass["Sensitivity"]
train_f1_score_rf <- (2 * train_precision_rf * train_recall_rf) / (train_precision_rf + train_recall_rf)

print("Train:")
train_conf_matrix_rf$overall["Accuracy"]
cat("F1-score:", train_f1_score_rf, "\n")

# Make predictions on the test data
test_predictions_rf <- predict(rf_model, newdata = test_data, type = "class")


# Create a confusion matrix
test_conf_matrix_rf <- confusionMatrix(data = test_predictions_rf, 
                                reference = test_data$attrition)

precision_rf <- test_conf_matrix_rf$byClass["Pos Pred Value"]
recall_rf <- test_conf_matrix_rf$byClass["Sensitivity"]
f1_score_rf <- (2 * precision_rf * recall_rf) / (precision_rf + recall_rf)



print("Test:")
test_conf_matrix_rf$overall["Accuracy"]
cat("F1-score:", f1_score_rf, "\n")

```



### Features Importance

```{r}
# Get variable importances
rf_importance <- as.data.frame(importance(rf_model))

# Sort the importances in descending order
sorted_importance <- rf_importance[order(-rf_importance[, 1]), ]

# Keep the top 6 features
top_features_rf <- row.names(sorted_importance)[1:6]
top_importances_rf <- sorted_importance[, 1][1:6]

# Scale the importances
scaled_importances_rf <- top_importances_rf / max(top_importances_rf)

# Create a data frame for plotting
scaled_plot_data_rf <- data.frame(
  Feature = top_features_rf,
  Importance = scaled_importances_rf
)

# Create a bar plot
ggplot(scaled_plot_data_rf, aes(x = Feature, y = Importance, fill = Feature)) +
  geom_bar(stat = "identity") +
  labs(x = "Feature", y = "Scaled Importance", title = "Top 6 Scaled Variable Importances (Random Forest)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```



### Subset
```{r}
# Subset the training data to include only the specified features
subset_train_data <- train_data[, c("satisfaction_level", "last_evaluation_rating", "time_spend_company", "average_montly_hours", "projects_worked_on", "attrition")]

# Train the Random Forest model using the subset of columns
set.seed(123)  # for reproducibility
rf_model_subset <- randomForest(attrition ~ ., data = subset_train_data, importance = TRUE, ntree = 100)

# Print the model summary
print(rf_model_subset)

```
```{r}
# Make predictions on the training data using the subset model
train_predictions_rf_subset <- predict(rf_model_subset, newdata = train_data, type = "class")

# Create a confusion matrix for the training set
train_conf_matrix_rf_subset <- confusionMatrix(data = train_predictions_rf_subset, 
                                               reference = train_data$attrition)

train_precision_rf_subset <- train_conf_matrix_rf_subset$byClass["Pos Pred Value"]
train_recall_rf_subset <- train_conf_matrix_rf_subset$byClass["Sensitivity"]
train_f1_score_rf_subset <- (2 * train_precision_rf_subset * train_recall_rf_subset) / 
                            (train_precision_rf_subset + train_recall_rf_subset)

print("Train (Subset Model):")
train_conf_matrix_rf_subset$overall["Accuracy"]
cat("F1-score:", train_f1_score_rf_subset, "\n")

# Make predictions on the test data using the subset model
test_predictions_rf_subset <- predict(rf_model_subset, newdata = test_data, type = "class")

# Create a confusion matrix for the test set
test_conf_matrix_rf_subset <- confusionMatrix(data = test_predictions_rf_subset, 
                                             reference = test_data$attrition)

precision_rf_subset <- test_conf_matrix_rf_subset$byClass["Pos Pred Value"]
recall_rf_subset <- test_conf_matrix_rf_subset$byClass["Sensitivity"]
f1_score_rf_subset <- (2 * precision_rf_subset * recall_rf_subset) / 
                      (precision_rf_subset + recall_rf_subset)

print("Test (Subset Model):")
test_conf_matrix_rf_subset$overall["Accuracy"]
cat("F1-score:", f1_score_rf_subset, "\n")

```



### Tree
```{r}
# Limit the depth of the tree (e.g., to 3 levels)
rf_tree <- ctree(attrition ~ ., data = train_data, control = ctree_control(maxdepth = 3))

# Adjust the size of the plot
options(repr.plot.width = 22, repr.plot.height = 20)

# Plot the modified tree

plot(rf_tree, type = "simple", inner_panel = node_inner(rf_tree))

```
## Decision tree

```{r}
# Train the Decision Tree model
tree_model <- rpart(attrition ~ ., data = train_data, method = "class")

# Print the model summary
print(tree_model)
```

### Metrics
```{r}
# Make predictions on the training data
train_predictions_tree <- predict(tree_model, newdata = train_data, type = "class")



# Create a confusion matrix for the training set
train_conf_matrix_tree <- confusionMatrix(data = train_predictions_tree, 
                                     reference = train_data$attrition)

train_precision_tree <- train_conf_matrix_tree$byClass["Pos Pred Value"]
train_recall_tree <- train_conf_matrix_tree$byClass["Sensitivity"]
train_f1_score_tree <- (2 * train_precision_tree * train_recall_tree) / (train_precision_tree + train_recall_tree)

print("Train:")
train_conf_matrix_tree$overall["Accuracy"]
cat("F1-score:", train_f1_score_tree, "\n")

# Make predictions on the test data
test_predictions_tree <- predict(tree_model, newdata = test_data, type = "class")


# Create a confusion matrix
test_conf_matrix_tree <- confusionMatrix(data = test_predictions_tree, 
                                reference = test_data$attrition)

precision_tree <- test_conf_matrix_tree$byClass["Pos Pred Value"]
recall_tree <- test_conf_matrix_tree$byClass["Sensitivity"]
f1_score_tree <- (2 * precision_tree * recall_tree) / (precision_tree + recall_tree)



print("Test:")
test_conf_matrix_tree$overall["Accuracy"]
cat("F1-score:", f1_score_tree, "\n")

```



### Feauture Importance
```{r}
# Convert the named numeric vector to a data frame
feature_importance_tree <- as.data.frame(as.table(tree_model$variable.importance))

# Rename the columns for clarity
colnames(feature_importance_tree) <- c("Feature", "Importance")

# Sort the data frame by Importance values in descending order
feature_importance_tree <- feature_importance_tree[order(-feature_importance_tree$Importance), ]

# Keep only the first 6 features
top_features_tree <- feature_importance_tree[1:6, ]

# Scale the variable importances
scaled_importance_tree <- top_features_tree$Importance / max(top_features_tree$Importance)

# Create a data frame for plotting
scaled_plot_data_tree <- data.frame(Feature = top_features_tree$Feature, Importance = scaled_importance_tree)

# Create a bar plot
ggplot(scaled_plot_data_tree, aes(x = Feature, y = Importance, fill = Feature)) +
  geom_bar(stat = "identity") +
  labs(x = "Feature", y = "Scaled Importance", title = "Top 6 Scaled Variable Importances (Decision Tree)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```


### Subset
```{r}
# Create a subset of the training and test data with selected features
train_data_subset <- train_data[, c("satisfaction_level", "last_evaluation_rating", "time_spend_company", "average_montly_hours", "projects_worked_on","attrition")]
test_data_subset <- test_data[, c("satisfaction_level", "last_evaluation_rating", "time_spend_company", "average_montly_hours", "projects_worked_on","attrition")]

# Train the Decision Tree model on the subset of features
tree_model_subset <- rpart(attrition ~ ., data = train_data_subset, method = "class")



# Make predictions on the training data
train_predictions_tree_subset <- predict(tree_model_subset, newdata = train_data_subset, type = "class")

# Create a confusion matrix for the training set
train_conf_matrix_tree_subset <- confusionMatrix(data = train_predictions_tree_subset, 
                                                 reference = train_data$attrition)

train_precision_tree_subset <- train_conf_matrix_tree_subset$byClass["Pos Pred Value"]
train_recall_tree_subset <- train_conf_matrix_tree_subset$byClass["Sensitivity"]
train_f1_score_tree_subset <- (2 * train_precision_tree_subset * train_recall_tree_subset) / (train_precision_tree_subset + train_recall_tree_subset)

print("Train Subset:")
train_conf_matrix_tree_subset$overall["Accuracy"]
cat("F1-score:", train_f1_score_tree_subset, "\n")

# Make predictions on the test data
test_predictions_tree_subset <- predict(tree_model_subset, newdata = test_data_subset, type = "class")

# Create a confusion matrix for the test set
test_conf_matrix_tree_subset <- confusionMatrix(data = test_predictions_tree_subset, 
                                                reference = test_data$attrition)

precision_tree_subset <- test_conf_matrix_tree_subset$byClass["Pos Pred Value"]
recall_tree_subset <- test_conf_matrix_tree_subset$byClass["Sensitivity"]
f1_score_tree_subset <- (2 * precision_tree_subset * recall_tree_subset) / (precision_tree_subset + recall_tree_subset)

print("Test Subset:")
test_conf_matrix_tree_subset$overall["Accuracy"]
cat("F1-score:", f1_score_tree_subset, "\n")

```

### Tree
```{r}
# Visualize the Decision Tree
rpart.plot(tree_model, extra = 101)
```

## k-NN

```{r}
# Create a mapping of salary levels to ordinal values
salary_mapping <- c("low" = 1, "medium" = 2, "high" = 3)

# Use the mapping to encode 'salary' manually
train_data_knn <- train_data %>%
  mutate(
    salary = salary_mapping[salary]
  )

test_data_knn <- test_data %>%
  mutate(
    salary = salary_mapping[salary]
  )



# create Data Domain X for the TRAINING set
train_X = train_data_knn[-9]

# create Data Domain X for the TEST set
test_X= test_data_knn[-9]

# labels of the training set
train_Y = train_data_knn$attrition

# labels of the test set
test_Y = test_data_knn$attrition
```

### Metrics
```{r}
# Define the range of k values to evaluate
k_values <- c(1,3,5,7,9)

# Initialize an empty vector to store the accuracies
accuracies <- c()

# Compute the accuracy for each k value
for (k in k_values) {
  knn_prediction <- knn(train_X, test_X, train_Y, k = k)
  accuracy <- mean(knn_prediction == test_Y)
  accuracies <- c(accuracies, accuracy)
}

# Create a data frame with the k values and accuracies
acc_data <- data.frame(k = k_values, accuracy = accuracies)

# Initialize an empty vector to store the error rates
error_rates <- c()
k_values <- c(1,3,5,7,9)

for (i in acc_data$accuracy){
error_rate <- 1 - i
error_rates <- c(error_rates, error_rate)
}


# Create a data frame with the k values and error rates
err_data <- data.frame(k = k_values, error_rate = error_rates)

```

```{r}
# Plot the error rate plot
ggplot(err_data, aes(x = k, y = error_rate)) +
  geom_line() +
  geom_point() +
  labs(x = "k", y = "Error Rate") +
  ggtitle("Error Rate Plot for KNN")
```


We choose k=5

```{r}
# Make predictions on the training data
# Create a k-NN model
k_value <- 5  # Replace with your desired k value
five_nn_train <- knn(train_X, train_X, train_Y, k = k_value )

# Create a confusion matrix for the training set
train_conf_matrix_knn <- confusionMatrix(data = five_nn_train, reference = train_Y)

train_precision_knn <- train_conf_matrix_knn$byClass["Pos Pred Value"]
train_recall_knn <- train_conf_matrix_knn$byClass["Sensitivity"]
train_f1_score_knn <- (2 * train_precision_knn * train_recall_knn) / (train_precision_knn + train_recall_knn)

print("Train:")
train_conf_matrix_knn$overall["Accuracy"]
cat("F1-score:", train_f1_score_knn, "\n")



# Make predictions on the test data
five_nn_test <- knn(train_X, test_X, train_Y, k = k_value )

# Create a confusion matrix for the test set
test_conf_matrix_knn <- confusionMatrix(data = five_nn_test, reference = test_Y)

precision_knn <- test_conf_matrix_knn$byClass["Pos Pred Value"]
recall_knn <- test_conf_matrix_knn$byClass["Sensitivity"]
f1_score_knn <- (2 * precision_knn * recall_knn) / (precision_knn + recall_knn)



print("Test:")
test_conf_matrix_knn$overall["Accuracy"]
cat("F1-score:", f1_score_knn, "\n")

```



### Subset (features of the tree)
```{r}
# Create a subset of the training and test data with selected features
train_data_knn_subset <- train_data_knn[, c("satisfaction_level", "last_evaluation_rating", "time_spend_company", "average_montly_hours", "projects_worked_on")]
test_data_knn_subset <- test_data_knn[, c("satisfaction_level", "last_evaluation_rating", "time_spend_company", "average_montly_hours", "projects_worked_on")]


# Define k value (5)
k_value <- 5


# Train the k-NN model on the subset of features
five_nn_train_subset <- knn(train_data_knn_subset, train_data_knn_subset, train_data_knn$attrition, k = k_value)

# Create a confusion matrix for the training set
train_conf_matrix_knn_subset <- confusionMatrix(data = five_nn_train_subset, reference = train_data_knn$attrition)

train_precision_knn_subset <- train_conf_matrix_knn_subset$byClass["Pos Pred Value"]
train_recall_knn_subset <- train_conf_matrix_knn_subset$byClass["Sensitivity"]
train_f1_score_knn_subset <- (2 * train_precision_knn_subset * train_recall_knn_subset) / (train_precision_knn_subset + train_recall_knn_subset)

print("Train:")
train_conf_matrix_knn_subset$overall["Accuracy"]
cat("F1-score:", train_f1_score_knn_subset, "\n")



# Make predictions on the test data
five_nn_test_subset <- knn(test_data_knn_subset, test_data_knn_subset, test_data_knn$attrition, k = k_value )

# Create a confusion matrix for the test set
test_conf_matrix_knn_subset  <- confusionMatrix(data = five_nn_test_subset, reference = test_data_knn$attrition)

precision_knn_subset <- test_conf_matrix_knn_subset$byClass["Pos Pred Value"]
recall_knn_subset <- test_conf_matrix_knn_subset$byClass["Sensitivity"]
f1_score_knn_subset <- (2 * precision_knn_subset * recall_knn_subset) / (precision_knn_subset + recall_knn_subset)



print("Test:")
test_conf_matrix_knn_subset$overall["Accuracy"]
cat("F1-score:", f1_score_knn_subset, "\n")

```


# Models Comparision

##Full Models

### ROC-AUC
```{r}
# Calculate ROC-AUC
roc_obj_knn <- roc(test_data$attrition, as.numeric(five_nn_test))
roc_obj_tree <- roc(test_data$attrition,as.numeric(test_predictions_tree))
roc_obj_rf <- roc(test_data$attrition,as.numeric(test_predictions_rf))
roc_obj_log <- roc(response = test_data$attrition, predictor = test_predictions_log)
```


```{r}
plot.roc(roc_obj_log, col = "blue", print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.3, print.auc.cex = 1.2, print.auc.col = "blue")
plot.roc(roc_obj_rf, col = "red", add = TRUE, print.auc = TRUE, print.auc.x = 0.6, print.auc.y = 0.4, print.auc.cex = 1.2, print.auc.col = "red")
plot.roc(roc_obj_tree, col = "green", add = TRUE, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.5, print.auc.cex = 1.2, print.auc.col = "green")
plot.roc(roc_obj_knn, col = "purple", add = TRUE, print.auc = TRUE, print.auc.x = 0.4, print.auc.y = 0.6, print.auc.cex = 1.2, print.auc.col = "purple")

# Add legend to the plot
legend("bottomright", legend = c("Logistic Regression", "Random Forest", "Decision Tree", "k-NN"),
       col = c("blue", "red", "green", "purple"), lty = 1, cex = 0.8)

```

### Precision-Recall
```{r}
precision_points_log <- c(1, precision_log, 0)
recall_points_log <- c(0, recall_log, 1)

precision_points_rf <- c(1, precision_rf, 0)
recall_points_rf <- c(0, recall_rf, 1)


precision_points_tree <- c(1, precision_tree, 0)
recall_points_tree <- c(0, recall_tree, 1)

precision_points_knn <- c(1, precision_knn, 0)
recall_points_knn <- c(0, recall_knn, 1)


# Create a new plot for Precision-Recall curves
plot(recall_points_knn, precision_points_knn, type = "b", xlim = c(0, 1), ylim = c(0, 1),
     xlab = "Recall", ylab = "Precision", main = "Precision-Recall Curve", col = "blue")
lines(recall_points_tree, precision_points_tree, type = "b", col = "green")
lines(recall_points_rf, precision_points_rf, type = "b", col = "red")
lines(recall_points_log, precision_points_log, type = "b", col = "purple")

# Add legend to the plot
legend("bottomright", legend = c("Logistic Regression", "Random Forest","Decision Tree", "k-NN"),
       col = c("blue", "red", "green", "purple"), lty = 1, cex = 0.8)

# Add horizontal and vertical lines for reference
abline(h = 0.5, col = "red", lty = 2)  # Horizontal line at y = 0.5 for reference
abline(v = 0.5, col = "red", lty = 2)  # Vertical line at x = 0.5 for reference

```


### Accuracy
```{r}
# Extract accuracy values from the confusion matrices

# For logistic regression
train_acc_log <- train_conf_matrix_log$overall["Accuracy"]
test_acc_log <- test_conf_matrix_log$overall["Accuracy"]

# For random forest
train_acc_rf <- train_conf_matrix_rf$overall["Accuracy"]
test_acc_rf <- test_conf_matrix_rf$overall["Accuracy"]

# For decision tree
train_acc_tree <- train_conf_matrix_tree$overall["Accuracy"]
test_acc_tree <- test_conf_matrix_tree$overall["Accuracy"]

# For k-NN
train_acc_knn <- train_conf_matrix_knn$overall["Accuracy"]
test_acc_knn <- test_conf_matrix_knn$overall["Accuracy"]

# Create a vector of model names
model_names <- c("Logistic Regression", "Random Forest", "Decision Tree", "k-NN")

# Create vectors for train and test accuracies
train_acc <- c(train_acc_log, train_acc_rf, train_acc_tree, train_acc_knn)
test_acc <- c(test_acc_log, test_acc_rf, test_acc_tree, test_acc_knn)

model_accuracies <- rbind(train_acc, test_acc)

# Bar plot
barplot(model_accuracies, beside = TRUE, col = c("orange", "green"), names.arg = model_names, 
        ylim = c(0, 1), main = "Training and Testing Accuracies by Model",
        xlab = "Model", ylab = "Accuracy")
legend("topleft", legend = c("Train", "Test"), fill = c("orange", "green"))

```


### Features Importance

```{r}
# Define custom colors for each model
custom_colors <- c("Logistic Regression" = "blue", "Random Forest" = "red", "Decision Tree" = "green")

# Combine the scaled data frames for all models
combined_scaled_importance <- rbind(
  cbind(Model = "Logistic Regression", plot_data_log),
  cbind(Model = "Random Forest", scaled_plot_data_rf),
  cbind(Model = "Decision Tree", scaled_plot_data_tree)
)

# Ensure there are always three bars for each variable
combined_scaled_importance <- combined_scaled_importance %>%
  complete(Model, Feature, fill = list(Importance = 0))

# Create a bar plot with dodge for different models and custom colors
ggplot(combined_scaled_importance, aes(x = Feature, y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Feature", y = "Scaled Importance", title = "Top Variable Importances by Model") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = custom_colors)  # Set custom colors here


```
Satisfaction Level: Across all three models (Logistic Regression, Random Forest, and Decision Tree), the "Satisfaction Level" is consistently identified as the most important feature. This suggests that employee satisfaction level plays a critical role in predicting attrition in the dataset.

Time Spent at the Company: In both Random Forest and Decision Tree models, the "Time Spent at the Company" is identified as the second most important feature. This indicates that the number of years an employee has spent with the company is a significant predictor of attrition.

Salary: In the Logistic Regression model, "Salary" is the second most important feature. However, in the Random Forest and Decision Tree models, it has a lower importance score. This discrepancy might be due to the differences in how these models handle categorical variables or non-linear relationships.

Other Features: "Last Evaluation Rating," "Average Monthly Hours," and "Projects Worked On" have varying importance levels across the models. These features are generally considered less important than satisfaction level and time spent at the company.

Promotion in the Last 5 Years: The feature "Promotion in the Last 5 Years" is relatively important in the Logistic Regression model but is not as prominent in the Random Forest and Decision Tree models. This suggests that promotions might not be a consistent predictor of attrition across all models.

In summary, employee satisfaction level and the amount of time spent at the company are strong indicators of attrition across all models. However, the importance of other features can vary depending on the modeling technique used. 


## Subset Models

### ROC-AUC
```{r}
# Calculate ROC-AUC
roc_obj_knn_subset <- roc(test_data$attrition, as.numeric(five_nn_test_subset))
roc_obj_tree_subset <- roc(test_data$attrition,as.numeric(test_predictions_tree_subset))
roc_obj_rf_subset <- roc(test_data$attrition,as.numeric(test_predictions_rf_subset))
roc_obj_log_subset <- roc(test_data$attrition,as.numeric(test_predictions_log_subset))

plot.roc(roc_obj_log_subset, col = "blue", print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.3, print.auc.cex = 1.2, print.auc.col = "blue")
plot.roc(roc_obj_rf_subset, col = "red", add = TRUE, print.auc = TRUE, print.auc.x = 0.6, print.auc.y = 0.4, print.auc.cex = 1.2, print.auc.col = "red")
plot.roc(roc_obj_tree_subset, col = "green", add = TRUE, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.5, print.auc.cex = 1.2, print.auc.col = "green")
plot.roc(roc_obj_knn_subset, col = "purple", add = TRUE, print.auc = TRUE, print.auc.x = 0.4, print.auc.y = 0.6, print.auc.cex = 1.2, print.auc.col = "purple")

# Add legend to the plot
legend("bottomright", legend = c("Logistic Regression", "Random Forest", "Decision Tree", "k-NN"),
       col = c("blue", "red", "green", "purple"), lty = 1, cex = 0.8)

```
### Precision-Recall
```{r}
precision_points_log_sub <- c(1, precision_log_subset, 0)
recall_points_log_sub <- c(0, recall_log_subset, 1)

precision_points_rf_sub <- c(1, precision_rf_subset, 0)
recall_points_rf_sub <- c(0, recall_rf_subset, 1)


precision_points_tree_sub <- c(1, precision_tree_subset, 0)
recall_points_tree_sub <- c(0, recall_tree_subset, 1)

precision_points_knn_sub <- c(1, precision_knn_subset, 0)
recall_points_knn_sub <- c(0, recall_knn_subset, 1)


# Create a new plot for Precision-Recall curves
plot(recall_points_knn_sub, precision_points_knn_sub, type = "b", xlim = c(0, 1), ylim = c(0, 1),
     xlab = "Recall", ylab = "Precision", main = "Precision-Recall Curve", col = "blue")
lines(recall_points_tree_sub, precision_points_tree_sub, type = "b", col = "green")
lines(recall_points_rf_sub, precision_points_rf_sub, type = "b", col = "red")
lines(recall_points_log_sub, precision_points_log_sub, type = "b", col = "purple")

# Add legend to the plot
legend("bottomright", legend = c("Logistic Regression", "Random Forest","Decision Tree", "k-NN"),
       col = c("blue", "red", "green", "purple"), lty = 1, cex = 0.8)

# Add horizontal and vertical lines for reference
abline(h = 0.5, col = "red", lty = 2)  # Horizontal line at y = 0.5 for reference
abline(v = 0.5, col = "red", lty = 2)  # Vertical line at x = 0.5 for reference
```


### Accuracy
```{r}
# Extract accuracy values from the confusion matrices

# For logistic regression
train_acc_log_sub <- train_conf_matrix_log_subset$overall["Accuracy"]
test_acc_log_sub <- test_conf_matrix_log_subset$overall["Accuracy"]

# For random forest
train_acc_rf_sub <- train_conf_matrix_rf_subset$overall["Accuracy"]
test_acc_rf_sub <- test_conf_matrix_rf_subset$overall["Accuracy"]

# For decision tree
train_acc_tree_sub <- train_conf_matrix_tree_subset$overall["Accuracy"]
test_acc_tree_sub <- test_conf_matrix_tree_subset$overall["Accuracy"]

# For k-NN
train_acc_knn_sub <- train_conf_matrix_knn_subset$overall["Accuracy"]
test_acc_knn_sub <- test_conf_matrix_knn_subset$overall["Accuracy"]

# Create a vector of model names
model_names_sub <- c("Logistic Sub", "RandomForest Sub", "DecisionTree Sub", "k-NN Sub")

# Create vectors for train and test accuracies
train_acc_sub <- c(train_acc_log_sub, train_acc_rf_sub, train_acc_tree_sub, train_acc_knn_sub)
test_acc_sub <- c(test_acc_log_sub, test_acc_rf_sub, test_acc_tree_sub, test_acc_knn_sub)

model_accuracies_sub <- rbind(train_acc_sub, test_acc_sub)

# Bar plot
barplot(model_accuracies_sub, beside = TRUE, col = c("orange", "green"), names.arg = model_names_sub, 
        ylim = c(0, 1), main = "Training and Testing Accuracies by SubModel",
        xlab = "Model", ylab = "Accuracy" ,las = 1)
legend("topleft", legend = c("Train", "Test"), fill = c("orange", "green"))
```



# Unsupervised learning
## Data Preprocessing 
```{r}
employee_pca <- employee
# Convert 'salary' column to numeric
employee_pca$salary <- ifelse(employee_pca$salary == "low", 1,
                         ifelse(employee_pca$salary == "medium", 2,
                                ifelse(employee_pca$salary == "high", 3, NA)))
```


```{r}
# One_hot encoding department
employee_pca <- employee_pca %>%
  dummy_cols(select_columns = "department")

```

```{r}
# Remove the "department" column
employee_pca <- employee_pca %>%
  select(-department)

```

```{r}
str(employee_pca)
```
```{r}
employee_pca <- employee_pca %>%
  select(-department_admin_and_managment)
```

## PCA
```{r}
pca_results <- prcomp(employee_pca[,-9],
                      scale = TRUE,
                      center = TRUE)
summary(pca_results)
```
```{r}
pc_var <- pca_results$sdev ** 2
pc_var
```


Percentage of variance explained by the components.
```{r}
explained_var <- pc_var / sum(pc_var)
explained_var
```

The first 5 components explain the 73% of the variance.
```{r}
sum(explained_var[1:5])
```


```{r}
fviz_eig(pca_results, addlabels = TRUE)
```

```{r}
plot(cumsum(explained_var), type = "o", ylab = "Cumulative Exp. Var.", 
     xlab = "Principal Component", col = "brown3",
     main = "Cumulative Variance Explained")
axis(side = 1, at = 1:10, labels = 1:10)
```
### PCA scores
```{r}
# Extract the loadings for the first 5 PCs
loadings <- pca_results$rotation[, 1:5]

# Create a heatmap
heatmap_data <- as.data.frame(loadings)
heatmap_data$Variable <- rownames(heatmap_data)
heatmap_data <- melt(heatmap_data, id.vars = "Variable")

ggplot(heatmap_data, aes(x = variable, y = Variable, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Variable Scores on Principal Components (PC1 to PC5)",
       x = "Principal Component",
       y = "Variable")


```




## k-Means Clustering
```{r}
# Access the principal components from the PCA results
pc1 <- pca_results$x[, 1]
pc2 <- pca_results$x[, 2]
pc3 <- pca_results$x[, 3]
pc4 <- pca_results$x[, 4]
pc5 <- pca_results$x[, 5]

# Create a data frame with the selected principal components
pc_data <- data.frame(PC1 = pc1, PC2 = pc2, PC3 = pc3, PC4 = pc4, PC5 = pc5)
```

```{r}
# Create a matrix from your PCA data
data_matrix <- as.matrix(pc_data)

# Initialize an empty vector to store within-cluster sum of squares
wss <- vector("double", length = 10)  # Assuming up to 10 clusters

# Calculate within-cluster sum of squares for different k values
for (i in 1:10) {
  kmeans_model <- kmeans(data_matrix, centers = i, nstart = 10)
  wss[i] <- kmeans_model$tot.withinss
}

# Create a data frame for the elbow plot
elbow_data <- data.frame(K = 1:10, WSS = wss)

# Plot the elbow plot
ggplot(elbow_data, aes(x = K, y = WSS)) +
  geom_line() +
  geom_point() +
  labs(x = "Number of Clusters", y = "Total Within Sum of Squares", 
       title = "Elbow Method for Optimal K") +
  geom_vline(xintercept = 3, linetype = 1, linewidth = 1, color = "red") +
  theme_minimal()



```


we choose k=3

```{r}
set.seed(2)
k <- 3  
# Perform K-Means clustering
kmeans_result <- kmeans(pc_data, centers = k)
```


```{r}
# Cluster assignments for each data point
cluster_assignments <- kmeans_result$cluster

# Add the cluster assignments to original data 
employee_pca$cluster <- cluster_assignments

# Add the cluster assignments to pc_data 
pc_data$cluster <- cluster_assignments
```

### Clusters
```{r}
# Define a color mapping for clusters
cluster_colors <- c("1" = "firebrick3", "2" = "#00AFBB", "3" = "#E7B800")

# Create a cluster plot with color mapping
cluster_plot <- fviz_cluster(kmeans_result, 
                             data = pc_data,
                             palette = cluster_colors,
                             geom = "point",
                             ellipse.type = "convex",
                             ggtheme = theme_minimal(),
                             xlab = "PC1",
                             ylab = "PC2") +
               ggtitle("Clusters Plot using first two components") +
               theme(plot.title = element_text(hjust = 0.5))

# Display the cluster plot
print(cluster_plot)



```

```{r}
# Calculate cluster means
cluster_means <- employee_pca %>%
  group_by(cluster) %>%
  summarise_all(list(mean = ~mean(., na.rm = TRUE)))


# Standardize the numeric columns (excluding cluster)
cluster_means <- cluster_means %>%
  mutate_at(vars(-cluster), scale)

# Convert the data to long format for plotting
cluster_means_long <- cluster_means %>%
  pivot_longer(-cluster, names_to = "variable", values_to = "mean_value")

# Create a parallel coordinates plot with color mapping
ggplot(cluster_means_long, aes(x = variable, y = mean_value, color = factor(cluster))) +
  geom_line(aes(group = cluster), alpha = 0.7, linewidth = 3) +
  labs(x = "Variable", y = "Mean Value", title = "Cluster Profiles") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        plot.title = element_text(size = 16),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10)) +
  scale_color_manual(values = cluster_colors)


```









